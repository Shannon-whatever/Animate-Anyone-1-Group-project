# APAI3010 Group Project - Animate Anyone: Reproduction, Analysis, and Extensions

## Description:
This repository contains our reproduction, analysis, and extension of the [Animate Anyone](https://arxiv.org/pdf/2311.17117) pose-driven character animation framework. We reproduced the model using Moore Threads’ unofficial implementation, evaluated its performance on standard benchmarks and a custom dataset, and explored extensions to improve facial animation. Our work highlights both the strengths and limitations of the current state-of-the-art in image-to-video character animation.

## Group Members:
- Li Ka Lam (u3605873@connect.hku.hk)
- Chen Shiyang (whatever@connect.hku.hk)
- Chechneva Kseniia (u3646503@connect.hku.hk)
  
## Key Components:

### Reproduction:

We faithfully reproduced the Animate Anyone framework by leveraging Moore Threads’ unofficial implementation, following the original pipeline for pose-driven character animation. Our experiments were conducted on public benchmarks—including the UBC Fashion and TikTok datasets—as well as a custom dataset featuring diverse human and non-human characters. The reproduction process involved replicating the model’s inference workflow, evaluating its performance in terms of temporal consistency, detail preservation, and generalization ability, and comparing our results with those reported in the original paper. This stage allowed us to systematically assess the strengths and shortcomings of the official approach in real-world scenarios.

You may check the datasets(videos, reference images and generated pose seqeunces) here: [Dataset (googledrive)](https://drive.google.com/drive/folders/1vCCnIUjNr7X9uojeLuJJ0cZKdxO32Kq2?usp=drive_link)

Here are some reproduction results we generated.

#### UBC Fashion dataset: detail preservation 

![UBC Examples (png)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/blob/main/Figures/UBC_fashion_comparison.png)

[UBC Examples (mp4)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/tree/main/demos/ubc)

#### TikTok Dataset: temporal consistency

![TikTok Examples (png)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/blob/main/Figures/TikTok_comparison.png)

[TikTok Examples (mp4)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/tree/main/demos/tiktok)

#### Limitations

- Facial Expression Limitation
- Suspected Overfitting or Implicit Biases

#### Robustness Evaluation

To better assess generalization and robustness, we constructed a custom dataset and evaluated the model on this new data without any additional pre-training. Below are example reference images from our custom evaluation set, featuring diverse subjects—including real humans, robots, cartoons, 3D models, and artistic styles—used to test the model’s generalization and robustness.

![Customized Dataset Summary (png)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/blob/main/Figures/Appendix_2/ref_img_summary.png)

[Customized Dataset (googledrive)](https://drive.google.com/drive/folders/1D1ZS7Mx8JDG0yUh_7au0quZRPT5DtUSv?usp=drive_link)

[Results (mp4)](https://github.com/Shannon-whatever/Animate-Anyone-1-Group-project/tree/main/demos/Customised%20dataset)

- Our analysis shows that the model is ill-suited for animating non-human or highly stylized characters. In such cases, pose sequences generated by DWPose are frequently unreliable, leading to severe temporal flickering and unrecognizable outputs.
- Only sequences involving human figures yield usable results, and even then, the model’s performance is highly sensitive to the alignment of pose and scale between the reference and driving inputs. Mismatches in figure size or pose configuration routinely result in “motion ghosting,” unstable backgrounds, and various visual artifacts.
- Attempts to animate non-humanoid figures, such as robots, consistently fail to reconstruct facial and body features, producing outputs that are distorted and lacking in semantic coherence.

### Extensions & Innovations:

Based on the original Animate Anyone framework and our own observations, we explored the following directions to address its limitations and broaden its applicability:

- **Facial Animation Enhancement:** Building on the paper’s observation that maintaining fine-grained facial consistency is challenging, we experimented with integrating additional facial keypoints and region-specific attention into the pose-guided pipeline, aiming to improve dynamic facial expression synthesis.
- **Robustness to Diverse Characters:** Inspired by Animate Anyone’s claim to animate “arbitrary characters” with expanded training, we evaluated and documented failure cases on non-human and stylized datasets. This motivates future work in expanding training diversity and augmenting pose representations for better generalization.
- **Efficiency and Inference Optimization:** We streamlined the inference pipeline following suggestions on efficient temporal modeling from the original paper, optimizing runtime and memory usage without sacrificing output quality.

## Contributions:

- **Comprehensive Reproduction:** Replicated the Animate Anyone pipeline and validated performance on widely-used benchmarks and new, challenging datasets.
- **In-depth Analysis:** Provided both quantitative and qualitative comparisons, identifying the model’s strengths (detail preservation, temporal consistency) and its weaknesses (especially with out-of-distribution data and facial motion).


## Collaboration:
We welcome feedback, suggestions, and collaboration from the academic and open-source community. Please feel free to open issues, submit pull requests, or contact us with questions. For course-related inquiries, please contact u3605873@connect.hku.hk.
